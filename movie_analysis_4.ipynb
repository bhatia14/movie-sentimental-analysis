{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "movie_analysis_4",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "dhwNuEsF-aIl",
        "colab_type": "code",
        "outputId": "51e6e82e-e5df-4390-860e-7911cddc6692",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=False)\n",
        "!ls \"/content/gdrive/My Drive/Colab Notebooks\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "'2_Movie sentiment neural network.ipynb'\n",
            "'2 ways to struture your nn in keras.ipynb'\n",
            "'3_Movie sentiment neural network.ipynb'\n",
            " blend1.csv\n",
            " blend_conv_conv_6.csv\n",
            " blend.csv\n",
            " blend_gru_lstm_conv_7.csv\n",
            " blend_gru_lstm_conv.csv\n",
            " blend.gsheet\n",
            " blend_lstm_lstm_conv_5.csv\n",
            " blend_valid.csv\n",
            "'Copy of movie_analysis_6'\n",
            "'Custom VGG16.ipynb'\n",
            "'keras tokenizer.ipynb'\n",
            " movie_analysis_2\n",
            " movie_analysis_3\n",
            " movie_analysis_4\n",
            " movie_analysis_5\n",
            " movie_analysis_6\n",
            " movie_analysis_7\n",
            " movie_analysis.ipynb\n",
            "'Movie sentiment neural network.ipynb'\n",
            " project\n",
            " project_2.ipynb\n",
            " sentimental_movie_analysis\n",
            " sentiment-analysis-on-movie-reviews\n",
            " Untitled0.ipynb\n",
            " Untitled1.ipynb\n",
            " VGG16.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gvg76HOW_Uv_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "import gensim\n",
        "from gensim.test.utils import common_texts, get_tmpfile\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
        "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
        "from keras.models import Model, load_model\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
        "from keras import backend as K\n",
        "from keras.engine import InputSpec, Layer\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CKpjhqLeItOJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Getting All the Data"
      ]
    },
    {
      "metadata": {
        "id": "c3_Qy2_4-4x8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/sentiment-analysis-on-movie-reviews/train.tsv', sep=\"\\t\")\n",
        "test = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/sentiment-analysis-on-movie-reviews/test.tsv', sep=\"\\t\")\n",
        "sub = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/sentiment-analysis-on-movie-reviews/sampleSubmission.csv', sep=\",\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "McRVqFWII-lB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Catagories variation in dataset"
      ]
    },
    {
      "metadata": {
        "id": "ZfkvgZDRqnux",
        "colab_type": "code",
        "outputId": "45926df8-bace-4cdf-f479-5c9081adc41d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "cell_type": "code",
      "source": [
        "train.groupby(train.Sentiment)['Sentiment'].count()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment\n",
              "0     7072\n",
              "1    27273\n",
              "2    79582\n",
              "3    32927\n",
              "4     9206\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "HCPSXlhZJJPP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rrTBH4EIJIod",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Balancing the sample in data set, using Stratify for train and test split"
      ]
    },
    {
      "metadata": {
        "id": "infvuoFhyY9t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_no_2=train[~(train[\"Sentiment\"]==2)]\n",
        "train_with_2=train[(train[\"Sentiment\"]==2)]\n",
        "train_with_2 = train_with_2.sample(28000)\n",
        "frames=[train_no_2,train_with_2]\n",
        "train=pd.concat(frames)\n",
        "train, val_data = train_test_split(train, test_size = 0.2, stratify=train['Sentiment'], random_state=200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "puddsc4g0Cmv",
        "colab_type": "code",
        "outputId": "fd1a1a9d-d5f6-4baa-d261-9284ffb35e7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "cell_type": "code",
      "source": [
        "train.groupby(train.Sentiment)['Sentiment'].count()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment\n",
              "0     5658\n",
              "1    21818\n",
              "2    22400\n",
              "3    26341\n",
              "4     7365\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "id": "9ie3IJxq0Bk0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pb8-84ZZAEex",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y = train['Sentiment']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qkGbWmdsJm85",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Tokenizer for Phrase to text and sequence"
      ]
    },
    {
      "metadata": {
        "id": "QQHeWyzd_hbN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tk = Tokenizer(lower = True, filters='')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xsq6zUq3_mN2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_tokenized = tk.texts_to_sequences(train['Phrase'])\n",
        "test_tokenized = tk.texts_to_sequences(test['Phrase'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4ZctU7Z7AHBr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "max_len = 50\n",
        "X_train = pad_sequences(train_tokenized, maxlen = max_len)\n",
        "X_test = pad_sequences(test_tokenized, maxlen = max_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JucS9Ct6AUPk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_path = \"/content/gdrive/My Drive/Colab Notebooks/project/GoogleNews-vectors-negative300.bin\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6kG8zexCAWUe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embed_size = 300\n",
        "max_features = 30000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "peOzgLN9Uoqr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kXdyYy-bJ7_r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Importing Google Pretrained Gensim Model"
      ]
    },
    {
      "metadata": {
        "id": "UUN0YJndUmns",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "word_vectors = KeyedVectors.load_word2vec_format(embedding_path, binary=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gXHCk1D3mq-y",
        "colab_type": "code",
        "outputId": "acb76616-332a-4b68-9d65-e07d5512bdc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "cell_type": "code",
      "source": [
        "NUM_WORDS=20000\n",
        "tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n",
        "                      lower=True)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences_train = tokenizer.texts_to_sequences(list(train['Phrase'].values))\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 15161 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X4-D4rF_KOYw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Generating Embedding Matrix"
      ]
    },
    {
      "metadata": {
        "id": "LWVBHyGJqj8Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM=300\n",
        "vocabulary_size=min(len(word_index)+1,NUM_WORDS)\n",
        "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if i>=NUM_WORDS:\n",
        "        continue\n",
        "    try:\n",
        "        embedding_vector = word_vectors[word]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
        "\n",
        "del(word_vectors)\n",
        "\n",
        "from keras.layers import Embedding\n",
        "embedding_layer = Embedding(vocabulary_size,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            trainable=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eY_5eC41KVMR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "peSHUrq4KUjR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "One-Hot Encoder"
      ]
    },
    {
      "metadata": {
        "id": "DfsuTu-2AcEC",
        "colab_type": "code",
        "outputId": "47e0d626-85b6-4f01-a9c6-712e7484ec95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ohe = OneHotEncoder(sparse=False)\n",
        "y_ohe = ohe.fit_transform(y.values.reshape(-1, 1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "kdOSH4s9J0uK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7j2Cc8lRApqW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_model1(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
        "    file_path = \"my_model.hdf5\"\n",
        "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
        "                                  save_best_only = True, mode = \"min\")\n",
        "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
        "    \n",
        "    inp = Input(shape = (max_len,))\n",
        "    x = Embedding(15162, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
        "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
        "\n",
        "    x_gru = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
        "    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
        "    avg_pool1_gru = GlobalAveragePooling1D()(x1)\n",
        "    max_pool1_gru = GlobalMaxPooling1D()(x1)\n",
        "\n",
        "    x_lstm = Bidirectional(CuDNNLSTM (units, return_sequences = True))(x1)\n",
        "    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
        "    avg_pool1_lstm = GlobalAveragePooling1D()(x1)\n",
        "    max_pool1_lstm = GlobalMaxPooling1D()(x1)\n",
        "    \n",
        "    \n",
        "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool1_lstm, max_pool1_lstm])\n",
        "    #x = BatchNormalization()(x_gru)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
        "    x = Dense(5, activation = \"sigmoid\")(x)\n",
        "    model = Model(inputs = inp, outputs = x)\n",
        "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
        "    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.2, \n",
        "                        verbose = 1, callbacks = [check_point, early_stop])\n",
        "    model = load_model(file_path)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g1rKVeIRAshB",
        "colab_type": "code",
        "outputId": "3484b3cc-825c-44c3-b651-a3bbd9074d20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1234
        }
      },
      "cell_type": "code",
      "source": [
        "model1 = build_model1(lr = 1e-3, lr_d = 1e-10, units = 64, spatial_dr = 0.3, kernel_size1=3, kernel_size2=2, dense_units=32, dr=0.1, conv_size=32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 66865 samples, validate on 16717 samples\n",
            "Epoch 1/20\n",
            "66865/66865 [==============================] - 44s 665us/step - loss: 0.4975 - acc: 0.7654 - val_loss: 0.4502 - val_acc: 0.8009\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.45018, saving model to my_model.hdf5\n",
            "Epoch 2/20\n",
            "66865/66865 [==============================] - 36s 539us/step - loss: 0.4560 - acc: 0.7988 - val_loss: 0.4452 - val_acc: 0.8014\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.45018 to 0.44525, saving model to my_model.hdf5\n",
            "Epoch 3/20\n",
            "66865/66865 [==============================] - 30s 449us/step - loss: 0.4456 - acc: 0.8009 - val_loss: 0.4360 - val_acc: 0.8018\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.44525 to 0.43602, saving model to my_model.hdf5\n",
            "Epoch 4/20\n",
            "66865/66865 [==============================] - 31s 470us/step - loss: 0.4364 - acc: 0.8027 - val_loss: 0.4284 - val_acc: 0.8036\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.43602 to 0.42840, saving model to my_model.hdf5\n",
            "Epoch 5/20\n",
            "66865/66865 [==============================] - 33s 500us/step - loss: 0.4261 - acc: 0.8053 - val_loss: 0.4180 - val_acc: 0.8051\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.42840 to 0.41803, saving model to my_model.hdf5\n",
            "Epoch 6/20\n",
            "66865/66865 [==============================] - 32s 484us/step - loss: 0.4123 - acc: 0.8093 - val_loss: 0.4128 - val_acc: 0.8087\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.41803 to 0.41278, saving model to my_model.hdf5\n",
            "Epoch 7/20\n",
            "66865/66865 [==============================] - 36s 534us/step - loss: 0.3989 - acc: 0.8137 - val_loss: 0.4129 - val_acc: 0.8033\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.41278\n",
            "Epoch 8/20\n",
            "66865/66865 [==============================] - 33s 495us/step - loss: 0.3864 - acc: 0.8182 - val_loss: 0.3878 - val_acc: 0.8180\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.41278 to 0.38783, saving model to my_model.hdf5\n",
            "Epoch 9/20\n",
            "66865/66865 [==============================] - 35s 529us/step - loss: 0.3767 - acc: 0.8226 - val_loss: 0.3795 - val_acc: 0.8211\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.38783 to 0.37955, saving model to my_model.hdf5\n",
            "Epoch 10/20\n",
            "66865/66865 [==============================] - 34s 507us/step - loss: 0.3676 - acc: 0.8278 - val_loss: 0.3811 - val_acc: 0.8152\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.37955\n",
            "Epoch 11/20\n",
            "66865/66865 [==============================] - 33s 500us/step - loss: 0.3608 - acc: 0.8309 - val_loss: 0.3672 - val_acc: 0.8263\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.37955 to 0.36722, saving model to my_model.hdf5\n",
            "Epoch 12/20\n",
            "66865/66865 [==============================] - 34s 511us/step - loss: 0.3550 - acc: 0.8349 - val_loss: 0.3716 - val_acc: 0.8239\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.36722\n",
            "Epoch 13/20\n",
            "66865/66865 [==============================] - 30s 450us/step - loss: 0.3500 - acc: 0.8368 - val_loss: 0.3723 - val_acc: 0.8218\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.36722\n",
            "Epoch 14/20\n",
            "66865/66865 [==============================] - 34s 514us/step - loss: 0.3448 - acc: 0.8398 - val_loss: 0.3541 - val_acc: 0.8332\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.36722 to 0.35405, saving model to my_model.hdf5\n",
            "Epoch 15/20\n",
            "66865/66865 [==============================] - 32s 485us/step - loss: 0.3406 - acc: 0.8426 - val_loss: 0.3671 - val_acc: 0.8265\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.35405\n",
            "Epoch 16/20\n",
            "66865/66865 [==============================] - 30s 450us/step - loss: 0.3363 - acc: 0.8447 - val_loss: 0.3977 - val_acc: 0.8109\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.35405\n",
            "Epoch 17/20\n",
            "66865/66865 [==============================] - 32s 484us/step - loss: 0.3321 - acc: 0.8470 - val_loss: 0.3616 - val_acc: 0.8328\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.35405\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YgRMVnOSAwPZ",
        "colab_type": "code",
        "outputId": "41c25c34-2db3-4078-980d-5722275dbe4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1020
        }
      },
      "cell_type": "code",
      "source": [
        "model2 = build_model1(lr = 1e-3, lr_d = 1e-10, units = 128, spatial_dr = 0.5, kernel_size1=3, kernel_size2=2, dense_units=64, dr=0.2, conv_size=32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 66865 samples, validate on 16717 samples\n",
            "Epoch 1/20\n",
            "66865/66865 [==============================] - 47s 703us/step - loss: 0.4881 - acc: 0.7788 - val_loss: 0.4534 - val_acc: 0.8000\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.45335, saving model to my_model.hdf5\n",
            "Epoch 2/20\n",
            "66865/66865 [==============================] - 39s 590us/step - loss: 0.4563 - acc: 0.7986 - val_loss: 0.4595 - val_acc: 0.7971\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.45335\n",
            "Epoch 3/20\n",
            "66865/66865 [==============================] - 39s 577us/step - loss: 0.4495 - acc: 0.8003 - val_loss: 0.4428 - val_acc: 0.8011\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.45335 to 0.44284, saving model to my_model.hdf5\n",
            "Epoch 4/20\n",
            "66865/66865 [==============================] - 39s 578us/step - loss: 0.4445 - acc: 0.8008 - val_loss: 0.4491 - val_acc: 0.8007\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.44284\n",
            "Epoch 5/20\n",
            "66865/66865 [==============================] - 39s 579us/step - loss: 0.4392 - acc: 0.8016 - val_loss: 0.4349 - val_acc: 0.8033\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.44284 to 0.43493, saving model to my_model.hdf5\n",
            "Epoch 6/20\n",
            "66865/66865 [==============================] - 39s 576us/step - loss: 0.4318 - acc: 0.8029 - val_loss: 0.4290 - val_acc: 0.8040\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.43493 to 0.42895, saving model to my_model.hdf5\n",
            "Epoch 7/20\n",
            "66865/66865 [==============================] - 39s 580us/step - loss: 0.4228 - acc: 0.8054 - val_loss: 0.4457 - val_acc: 0.8026\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.42895\n",
            "Epoch 8/20\n",
            "66865/66865 [==============================] - 39s 581us/step - loss: 0.4133 - acc: 0.8078 - val_loss: 0.4684 - val_acc: 0.8074\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.42895\n",
            "Epoch 9/20\n",
            "66865/66865 [==============================] - 39s 579us/step - loss: 0.4047 - acc: 0.8112 - val_loss: 0.4733 - val_acc: 0.7993\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.42895\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "83sH7KV3BEf-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_model2(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
        "    file_path = \"my_model.hdf5\"\n",
        "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
        "                                  save_best_only = True, mode = \"min\")\n",
        "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
        "\n",
        "    inp = Input(shape = (max_len,))\n",
        "    x = Embedding(15162, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
        "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
        "\n",
        "    x_gru = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
        "    x_lstm = Bidirectional(CuDNNLSTM (units, return_sequences = True))(x_gru)\n",
        "    \n",
        "    x_conv1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
        "    avg_pool1_gru = GlobalAveragePooling1D()(x_conv1)\n",
        "    max_pool1_gru = GlobalMaxPooling1D()(x_conv1)\n",
        "\n",
        "    x_conv3 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
        "    avg_pool1_lstm = GlobalAveragePooling1D()(x_conv3)\n",
        "    max_pool1_lstm = GlobalMaxPooling1D()(x_conv3)\n",
        "    \n",
        "    \n",
        "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool1_lstm, max_pool1_lstm])\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
        "    x = Dense(5, activation = \"sigmoid\")(x)\n",
        "    model = Model(inputs = inp, outputs = x)\n",
        "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
        "    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.2, \n",
        "                        verbose = 1, callbacks = [check_point, early_stop])\n",
        "    model = load_model(file_path)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HVcPUEK1BGwg",
        "colab_type": "code",
        "outputId": "e3f7c915-5863-4e61-d088-2718331e5d92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1442
        }
      },
      "cell_type": "code",
      "source": [
        "model3 = build_model2(lr = 1e-4, lr_d = 0, units = 64, spatial_dr = 0.5, kernel_size1=4, kernel_size2=3, dense_units=32, dr=0.1, conv_size=32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 66865 samples, validate on 16717 samples\n",
            "Epoch 1/20\n",
            "66865/66865 [==============================] - 45s 669us/step - loss: 0.5602 - acc: 0.7401 - val_loss: 0.4738 - val_acc: 0.7985\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.47379, saving model to my_model.hdf5\n",
            "Epoch 2/20\n",
            "66865/66865 [==============================] - 41s 620us/step - loss: 0.4893 - acc: 0.7846 - val_loss: 0.4565 - val_acc: 0.7997\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.47379 to 0.45654, saving model to my_model.hdf5\n",
            "Epoch 3/20\n",
            "66865/66865 [==============================] - 31s 465us/step - loss: 0.4752 - acc: 0.7887 - val_loss: 0.4521 - val_acc: 0.8000\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.45654 to 0.45213, saving model to my_model.hdf5\n",
            "Epoch 4/20\n",
            "66865/66865 [==============================] - 29s 441us/step - loss: 0.4691 - acc: 0.7904 - val_loss: 0.4505 - val_acc: 0.8008\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.45213 to 0.45047, saving model to my_model.hdf5\n",
            "Epoch 5/20\n",
            "66865/66865 [==============================] - 31s 461us/step - loss: 0.4653 - acc: 0.7925 - val_loss: 0.4492 - val_acc: 0.8000\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.45047 to 0.44923, saving model to my_model.hdf5\n",
            "Epoch 6/20\n",
            "66865/66865 [==============================] - 30s 444us/step - loss: 0.4618 - acc: 0.7946 - val_loss: 0.4476 - val_acc: 0.8013\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.44923 to 0.44757, saving model to my_model.hdf5\n",
            "Epoch 7/20\n",
            "66865/66865 [==============================] - 31s 469us/step - loss: 0.4596 - acc: 0.7961 - val_loss: 0.4459 - val_acc: 0.8011\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.44757 to 0.44590, saving model to my_model.hdf5\n",
            "Epoch 8/20\n",
            "66865/66865 [==============================] - 30s 443us/step - loss: 0.4570 - acc: 0.7973 - val_loss: 0.4443 - val_acc: 0.8013\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.44590 to 0.44430, saving model to my_model.hdf5\n",
            "Epoch 9/20\n",
            "66865/66865 [==============================] - 31s 469us/step - loss: 0.4546 - acc: 0.7984 - val_loss: 0.4436 - val_acc: 0.8014\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.44430 to 0.44357, saving model to my_model.hdf5\n",
            "Epoch 10/20\n",
            "66865/66865 [==============================] - 30s 441us/step - loss: 0.4534 - acc: 0.7988 - val_loss: 0.4425 - val_acc: 0.8018\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.44357 to 0.44246, saving model to my_model.hdf5\n",
            "Epoch 11/20\n",
            "66865/66865 [==============================] - 32s 472us/step - loss: 0.4512 - acc: 0.7990 - val_loss: 0.4418 - val_acc: 0.8016\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.44246 to 0.44178, saving model to my_model.hdf5\n",
            "Epoch 12/20\n",
            "66865/66865 [==============================] - 30s 442us/step - loss: 0.4500 - acc: 0.7993 - val_loss: 0.4393 - val_acc: 0.8016\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.44178 to 0.43931, saving model to my_model.hdf5\n",
            "Epoch 13/20\n",
            "66865/66865 [==============================] - 30s 445us/step - loss: 0.4489 - acc: 0.7998 - val_loss: 0.4388 - val_acc: 0.8020\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.43931 to 0.43883, saving model to my_model.hdf5\n",
            "Epoch 14/20\n",
            "66865/66865 [==============================] - 30s 449us/step - loss: 0.4468 - acc: 0.8004 - val_loss: 0.4386 - val_acc: 0.8022\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.43883 to 0.43861, saving model to my_model.hdf5\n",
            "Epoch 15/20\n",
            "66865/66865 [==============================] - 30s 443us/step - loss: 0.4459 - acc: 0.8002 - val_loss: 0.4362 - val_acc: 0.8028\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.43861 to 0.43620, saving model to my_model.hdf5\n",
            "Epoch 16/20\n",
            "66865/66865 [==============================] - 32s 472us/step - loss: 0.4448 - acc: 0.8004 - val_loss: 0.4358 - val_acc: 0.8022\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.43620 to 0.43580, saving model to my_model.hdf5\n",
            "Epoch 17/20\n",
            "66865/66865 [==============================] - 30s 444us/step - loss: 0.4438 - acc: 0.8011 - val_loss: 0.4349 - val_acc: 0.8030\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.43580 to 0.43486, saving model to my_model.hdf5\n",
            "Epoch 18/20\n",
            "66865/66865 [==============================] - 33s 497us/step - loss: 0.4421 - acc: 0.8010 - val_loss: 0.4342 - val_acc: 0.8034\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.43486 to 0.43424, saving model to my_model.hdf5\n",
            "Epoch 19/20\n",
            "66865/66865 [==============================] - 30s 446us/step - loss: 0.4414 - acc: 0.8018 - val_loss: 0.4342 - val_acc: 0.8034\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.43424 to 0.43420, saving model to my_model.hdf5\n",
            "Epoch 20/20\n",
            "66865/66865 [==============================] - 32s 475us/step - loss: 0.4400 - acc: 0.8021 - val_loss: 0.4316 - val_acc: 0.8030\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.43420 to 0.43162, saving model to my_model.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Prl3ERR8BI29",
        "colab_type": "code",
        "outputId": "0a921b6c-0945-46e4-f705-57f82733ffa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1442
        }
      },
      "cell_type": "code",
      "source": [
        "model4 = build_model2(lr = 1e-3, lr_d = 0, units = 64, spatial_dr = 0.5, kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.3, conv_size=32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 66865 samples, validate on 16717 samples\n",
            "Epoch 1/20\n",
            "66865/66865 [==============================] - 48s 720us/step - loss: 0.5110 - acc: 0.7645 - val_loss: 0.4501 - val_acc: 0.7999\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.45013, saving model to my_model.hdf5\n",
            "Epoch 2/20\n",
            "66865/66865 [==============================] - 41s 618us/step - loss: 0.4607 - acc: 0.7987 - val_loss: 0.4497 - val_acc: 0.8004\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.45013 to 0.44966, saving model to my_model.hdf5\n",
            "Epoch 3/20\n",
            "66865/66865 [==============================] - 33s 499us/step - loss: 0.4520 - acc: 0.8000 - val_loss: 0.4436 - val_acc: 0.8017\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.44966 to 0.44358, saving model to my_model.hdf5\n",
            "Epoch 4/20\n",
            "66865/66865 [==============================] - 31s 463us/step - loss: 0.4480 - acc: 0.8005 - val_loss: 0.4419 - val_acc: 0.8026\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.44358 to 0.44194, saving model to my_model.hdf5\n",
            "Epoch 5/20\n",
            "66865/66865 [==============================] - 30s 445us/step - loss: 0.4441 - acc: 0.8013 - val_loss: 0.4362 - val_acc: 0.8011\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.44194 to 0.43624, saving model to my_model.hdf5\n",
            "Epoch 6/20\n",
            "66865/66865 [==============================] - 31s 461us/step - loss: 0.4390 - acc: 0.8016 - val_loss: 0.4281 - val_acc: 0.8026\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.43624 to 0.42805, saving model to my_model.hdf5\n",
            "Epoch 7/20\n",
            "66865/66865 [==============================] - 30s 446us/step - loss: 0.4329 - acc: 0.8025 - val_loss: 0.4238 - val_acc: 0.8043\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.42805 to 0.42382, saving model to my_model.hdf5\n",
            "Epoch 8/20\n",
            "66865/66865 [==============================] - 31s 466us/step - loss: 0.4253 - acc: 0.8046 - val_loss: 0.4144 - val_acc: 0.8063\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.42382 to 0.41440, saving model to my_model.hdf5\n",
            "Epoch 9/20\n",
            "66865/66865 [==============================] - 30s 449us/step - loss: 0.4167 - acc: 0.8076 - val_loss: 0.4074 - val_acc: 0.8082\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.41440 to 0.40737, saving model to my_model.hdf5\n",
            "Epoch 10/20\n",
            "66865/66865 [==============================] - 31s 460us/step - loss: 0.4105 - acc: 0.8085 - val_loss: 0.4022 - val_acc: 0.8099\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.40737 to 0.40217, saving model to my_model.hdf5\n",
            "Epoch 11/20\n",
            "66865/66865 [==============================] - 30s 442us/step - loss: 0.4040 - acc: 0.8105 - val_loss: 0.4027 - val_acc: 0.8108\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.40217\n",
            "Epoch 12/20\n",
            "66865/66865 [==============================] - 32s 474us/step - loss: 0.3991 - acc: 0.8129 - val_loss: 0.3907 - val_acc: 0.8123\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.40217 to 0.39074, saving model to my_model.hdf5\n",
            "Epoch 13/20\n",
            "66865/66865 [==============================] - 30s 451us/step - loss: 0.3931 - acc: 0.8152 - val_loss: 0.3816 - val_acc: 0.8174\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.39074 to 0.38158, saving model to my_model.hdf5\n",
            "Epoch 14/20\n",
            "66865/66865 [==============================] - 29s 441us/step - loss: 0.3891 - acc: 0.8167 - val_loss: 0.3826 - val_acc: 0.8186\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.38158\n",
            "Epoch 15/20\n",
            "66865/66865 [==============================] - 31s 463us/step - loss: 0.3840 - acc: 0.8197 - val_loss: 0.3722 - val_acc: 0.8219\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.38158 to 0.37220, saving model to my_model.hdf5\n",
            "Epoch 16/20\n",
            "66865/66865 [==============================] - 30s 445us/step - loss: 0.3819 - acc: 0.8205 - val_loss: 0.3737 - val_acc: 0.8213\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.37220\n",
            "Epoch 17/20\n",
            "66865/66865 [==============================] - 32s 472us/step - loss: 0.3769 - acc: 0.8225 - val_loss: 0.3760 - val_acc: 0.8222\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.37220\n",
            "Epoch 18/20\n",
            "66865/66865 [==============================] - 30s 448us/step - loss: 0.3745 - acc: 0.8237 - val_loss: 0.3659 - val_acc: 0.8271\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.37220 to 0.36594, saving model to my_model.hdf5\n",
            "Epoch 19/20\n",
            "66865/66865 [==============================] - 30s 447us/step - loss: 0.3717 - acc: 0.8256 - val_loss: 0.3632 - val_acc: 0.8282\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.36594 to 0.36318, saving model to my_model.hdf5\n",
            "Epoch 20/20\n",
            "66865/66865 [==============================] - 30s 452us/step - loss: 0.3683 - acc: 0.8273 - val_loss: 0.3621 - val_acc: 0.8265\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.36318 to 0.36214, saving model to my_model.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uInyHAvLBK_B",
        "colab_type": "code",
        "outputId": "2b911bd8-a7f1-4b5c-93f5-ddebdeee601a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1234
        }
      },
      "cell_type": "code",
      "source": [
        "model5 = build_model2(lr = 1e-3, lr_d = 1e-7, units = 64, spatial_dr = 0.3, kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.4, conv_size=64)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 66865 samples, validate on 16717 samples\n",
            "Epoch 1/20\n",
            "66865/66865 [==============================] - 40s 592us/step - loss: 0.5309 - acc: 0.7487 - val_loss: 0.4568 - val_acc: 0.8000\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.45681, saving model to my_model.hdf5\n",
            "Epoch 2/20\n",
            "66865/66865 [==============================] - 32s 483us/step - loss: 0.4633 - acc: 0.7988 - val_loss: 0.4469 - val_acc: 0.8000\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.45681 to 0.44689, saving model to my_model.hdf5\n",
            "Epoch 3/20\n",
            "66865/66865 [==============================] - 31s 462us/step - loss: 0.4539 - acc: 0.8001 - val_loss: 0.4509 - val_acc: 0.8004\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.44689\n",
            "Epoch 4/20\n",
            "66865/66865 [==============================] - 30s 454us/step - loss: 0.4473 - acc: 0.8008 - val_loss: 0.4426 - val_acc: 0.8029\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.44689 to 0.44258, saving model to my_model.hdf5\n",
            "Epoch 5/20\n",
            "66865/66865 [==============================] - 31s 469us/step - loss: 0.4397 - acc: 0.8018 - val_loss: 0.4332 - val_acc: 0.8037\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.44258 to 0.43321, saving model to my_model.hdf5\n",
            "Epoch 6/20\n",
            "66865/66865 [==============================] - 31s 460us/step - loss: 0.4290 - acc: 0.8043 - val_loss: 0.4243 - val_acc: 0.8070\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.43321 to 0.42426, saving model to my_model.hdf5\n",
            "Epoch 7/20\n",
            "66865/66865 [==============================] - 30s 455us/step - loss: 0.4157 - acc: 0.8070 - val_loss: 0.4258 - val_acc: 0.8044\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.42426\n",
            "Epoch 8/20\n",
            "66865/66865 [==============================] - 32s 477us/step - loss: 0.4039 - acc: 0.8106 - val_loss: 0.4015 - val_acc: 0.8075\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.42426 to 0.40150, saving model to my_model.hdf5\n",
            "Epoch 9/20\n",
            "66865/66865 [==============================] - 30s 454us/step - loss: 0.3930 - acc: 0.8143 - val_loss: 0.3963 - val_acc: 0.8103\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.40150 to 0.39627, saving model to my_model.hdf5\n",
            "Epoch 10/20\n",
            "66865/66865 [==============================] - 33s 490us/step - loss: 0.3850 - acc: 0.8171 - val_loss: 0.3814 - val_acc: 0.8181\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.39627 to 0.38141, saving model to my_model.hdf5\n",
            "Epoch 11/20\n",
            "66865/66865 [==============================] - 31s 461us/step - loss: 0.3786 - acc: 0.8206 - val_loss: 0.4160 - val_acc: 0.8047\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.38141\n",
            "Epoch 12/20\n",
            "66865/66865 [==============================] - 31s 458us/step - loss: 0.3713 - acc: 0.8242 - val_loss: 0.4413 - val_acc: 0.8015\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.38141\n",
            "Epoch 13/20\n",
            "66865/66865 [==============================] - 32s 476us/step - loss: 0.3645 - acc: 0.8277 - val_loss: 0.3769 - val_acc: 0.8211\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.38141 to 0.37687, saving model to my_model.hdf5\n",
            "Epoch 14/20\n",
            "66865/66865 [==============================] - 31s 459us/step - loss: 0.3591 - acc: 0.8304 - val_loss: 0.3579 - val_acc: 0.8273\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.37687 to 0.35786, saving model to my_model.hdf5\n",
            "Epoch 15/20\n",
            "66865/66865 [==============================] - 32s 483us/step - loss: 0.3550 - acc: 0.8329 - val_loss: 0.3625 - val_acc: 0.8284\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.35786\n",
            "Epoch 16/20\n",
            "66865/66865 [==============================] - 31s 457us/step - loss: 0.3512 - acc: 0.8353 - val_loss: 0.3593 - val_acc: 0.8294\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.35786\n",
            "Epoch 17/20\n",
            "66865/66865 [==============================] - 32s 477us/step - loss: 0.3468 - acc: 0.8375 - val_loss: 0.3632 - val_acc: 0.8279\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.35786\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f7gHVsbRBMrH",
        "colab_type": "code",
        "outputId": "36a511d3-b72d-4f2f-89ab-f76f1dd16d22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "pred1 = model1.predict(X_test, batch_size = 1024, verbose = 1)\n",
        "pred = pred1\n",
        "pred2 = model2.predict(X_test, batch_size = 1024, verbose = 1)\n",
        "pred += pred2\n",
        "pred3 = model3.predict(X_test, batch_size = 1024, verbose = 1)\n",
        "pred += pred3\n",
        "pred4 = model4.predict(X_test, batch_size = 1024, verbose = 1)\n",
        "pred += pred4\n",
        "pred5 = model5.predict(X_test, batch_size = 1024, verbose = 1)\n",
        "pred += pred5"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "66292/66292 [==============================] - 5s 80us/step\n",
            "66292/66292 [==============================] - 7s 107us/step\n",
            "66292/66292 [==============================] - 5s 83us/step\n",
            "66292/66292 [==============================] - 5s 82us/step\n",
            "66292/66292 [==============================] - 6s 87us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FjViJzXgBOTY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = np.round(np.argmax(pred, axis=1)).astype(int)\n",
        "sub['Sentiment'] = predictions\n",
        "sub.to_csv(\"/content/gdrive/My Drive/Colab Notebooks/lstm_lstm_conv.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ogN9eYr_WwwM",
        "colab_type": "code",
        "outputId": "d2d96a00-fa5d-421b-8946-692bb02bb739",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "cell_type": "code",
      "source": [
        "X_test.size"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3314600"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "metadata": {
        "id": "toi5ernYWxcC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "result_df = pd.read_csv(\"/content/gdrive/My Drive/Colab Notebooks/lstm_lstm_conv.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CRSKLA3bXEbV",
        "colab_type": "code",
        "outputId": "908b0848-ce79-4994-e510-dc36b2011ce3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2010
        }
      },
      "cell_type": "code",
      "source": [
        "result_df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>156061</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>156062</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>156063</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>156064</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>156065</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>156066</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>156067</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>156068</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>156069</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>156070</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>156071</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>156072</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>156073</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>156074</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>156075</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>156076</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>156077</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>156078</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>156079</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>156080</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>156081</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>156082</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>156083</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>156084</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>156085</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>156086</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>156087</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>156088</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>156089</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>156090</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66262</th>\n",
              "      <td>222323</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66263</th>\n",
              "      <td>222324</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66264</th>\n",
              "      <td>222325</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66265</th>\n",
              "      <td>222326</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66266</th>\n",
              "      <td>222327</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66267</th>\n",
              "      <td>222328</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66268</th>\n",
              "      <td>222329</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66269</th>\n",
              "      <td>222330</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66270</th>\n",
              "      <td>222331</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66271</th>\n",
              "      <td>222332</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66272</th>\n",
              "      <td>222333</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66273</th>\n",
              "      <td>222334</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66274</th>\n",
              "      <td>222335</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66275</th>\n",
              "      <td>222336</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66276</th>\n",
              "      <td>222337</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66277</th>\n",
              "      <td>222338</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66278</th>\n",
              "      <td>222339</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66279</th>\n",
              "      <td>222340</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66280</th>\n",
              "      <td>222341</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66281</th>\n",
              "      <td>222342</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66282</th>\n",
              "      <td>222343</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66283</th>\n",
              "      <td>222344</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66284</th>\n",
              "      <td>222345</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66285</th>\n",
              "      <td>222346</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66286</th>\n",
              "      <td>222347</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66287</th>\n",
              "      <td>222348</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66288</th>\n",
              "      <td>222349</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66289</th>\n",
              "      <td>222350</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66290</th>\n",
              "      <td>222351</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66291</th>\n",
              "      <td>222352</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>66292 rows  2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       PhraseId  Sentiment\n",
              "0        156061          3\n",
              "1        156062          3\n",
              "2        156063          2\n",
              "3        156064          3\n",
              "4        156065          2\n",
              "5        156066          2\n",
              "6        156067          2\n",
              "7        156068          2\n",
              "8        156069          2\n",
              "9        156070          2\n",
              "10       156071          2\n",
              "11       156072          2\n",
              "12       156073          2\n",
              "13       156074          2\n",
              "14       156075          2\n",
              "15       156076          1\n",
              "16       156077          2\n",
              "17       156078          1\n",
              "18       156079          1\n",
              "19       156080          2\n",
              "20       156081          2\n",
              "21       156082          2\n",
              "22       156083          1\n",
              "23       156084          2\n",
              "24       156085          2\n",
              "25       156086          2\n",
              "26       156087          2\n",
              "27       156088          2\n",
              "28       156089          1\n",
              "29       156090          2\n",
              "...         ...        ...\n",
              "66262    222323          2\n",
              "66263    222324          2\n",
              "66264    222325          1\n",
              "66265    222326          3\n",
              "66266    222327          3\n",
              "66267    222328          3\n",
              "66268    222329          1\n",
              "66269    222330          1\n",
              "66270    222331          2\n",
              "66271    222332          1\n",
              "66272    222333          1\n",
              "66273    222334          1\n",
              "66274    222335          1\n",
              "66275    222336          1\n",
              "66276    222337          2\n",
              "66277    222338          1\n",
              "66278    222339          2\n",
              "66279    222340          2\n",
              "66280    222341          1\n",
              "66281    222342          1\n",
              "66282    222343          1\n",
              "66283    222344          2\n",
              "66284    222345          2\n",
              "66285    222346          2\n",
              "66286    222347          2\n",
              "66287    222348          1\n",
              "66288    222349          1\n",
              "66289    222350          1\n",
              "66290    222351          2\n",
              "66291    222352          1\n",
              "\n",
              "[66292 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "metadata": {
        "id": "RTC9xAHrniKo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}